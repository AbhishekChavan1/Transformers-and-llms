# -*- coding: utf-8 -*-
"""baai/bge2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o6gZssWEFjkYB8CRHLhKn49nXm3yePc_
"""

!pip install transformers torch sentence-transformers datasets -q
!pip install mteb faiss-cpu sentence-transformers -q

# ==============================================================================
# Step 2: Imports and the Corrected, Definitive Custom Model
# ==============================================================================
import torch
import torch.nn as nn
import torch.nn.functional as F
from datasets import load_dataset
from torch.utils.data import DataLoader
from sentence_transformers import SentenceTransformer, InputExample, losses
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from sentence_transformers.models import Transformer
import math
from torch import nn, optim
from transformers import AutoTokenizer, AutoModel # Import AutoModel here
from mteb import MTEB
from sentence_transformers import SentencesDataset, evaluation

print("Libraries installed and imported successfully.")

# ----------------------
# Projection Head
# ----------------------
class ProjectionHead(nn.Module):
    def __init__(self, in_dim, hidden_dim=768, out_dim=512):
        super().__init__()
        self.proj = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, out_dim),
        )

    def forward(self, x):
        x = self.proj(x)
        x = F.normalize(x, p=2, dim=-1)  # L2 normalize
        return x

# ----------------------
# Attention Pooling
# ----------------------
class AttentionPooling(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.attn = nn.Linear(hidden_size, 1)

    def forward(self, hidden_states, mask):
        # hidden_states: [B, T, D], mask: [B, T]
        scores = self.attn(hidden_states).squeeze(-1)  # [B, T]
        scores = scores.masked_fill(mask == 0, -1e9)
        weights = torch.softmax(scores, dim=-1)        # [B, T]
        pooled = torch.bmm(weights.unsqueeze(1), hidden_states).squeeze(1)
        return pooled

# ----------------------
# Layerwise Pooling
# ----------------------
class LayerwisePooling(nn.Module):
    def __init__(self, num_layers):
        super().__init__()
        self.weights = nn.Parameter(torch.zeros(num_layers))

    def forward(self, all_hidden_states, mask):
        # all_hidden_states: list of [B, T, D]
        stacked = torch.stack(all_hidden_states, dim=0)   # [L, B, T, D]
        norm_w = torch.softmax(self.weights, dim=0)       # [L]
        combined = torch.sum(norm_w[:, None, None, None] * stacked, dim=0)

        # mean pooling after combining
        mask_exp = mask.unsqueeze(-1).expand(combined.size())
        pooled = (combined * mask_exp).sum(dim=1) / mask_exp.sum(dim=1).clamp(min=1e-9)
        return pooled

# ----------------------
# Custom Embedder
# ----------------------
class CustomBGEEmbedder(nn.Module):
    def __init__(self, model_name="BAAI/bge-base-en", pooling="mean", use_layerwise=False):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(model_name, output_hidden_states=True)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        hidden_size = self.encoder.config.hidden_size
        self.pooling = pooling
        self.attn_pool = AttentionPooling(hidden_size)
        self.layerwise = LayerwisePooling(num_layers=self.encoder.config.num_hidden_layers) if use_layerwise else None
        self.proj_head = ProjectionHead(in_dim=hidden_size, out_dim=512)

    def forward(self, texts):
        inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
        outputs = self.encoder(**inputs)
        hidden_states = outputs.last_hidden_state  # [B, T, D]
        mask = inputs["attention_mask"]

        # --- Pooling strategy ---
        if self.layerwise is not None:
            # weighted sum of all layers
            all_hidden = outputs.hidden_states[1:]  # exclude embedding layer
            pooled = self.layerwise(all_hidden, mask)
        elif self.pooling == "cls":
            pooled = hidden_states[:, 0]  # CLS token
        elif self.pooling == "mean":
            mask_exp = mask.unsqueeze(-1).expand(hidden_states.size())
            pooled = (hidden_states * mask_exp).sum(dim=1) / mask_exp.sum(dim=1).clamp(min=1e-9)
        elif self.pooling == "attention":
            pooled = self.attn_pool(hidden_states, mask)
        else:
            raise ValueError(f"Unknown pooling type: {self.pooling}")

        # --- Projection + L2 norm ---
        emb = self.proj_head(pooled)
        return emb

# Wrap it into a SentenceTransformer-like interface
class CustomSentenceTransformer(SentenceTransformer):
    def __init__(self, model_name="BAAI/bge-base-en", pooling="mean", use_layerwise=False, device="cuda"):
        # Call the parent class's __init__ first
        super().__init__()
        self.model = CustomBGEEmbedder(model_name, pooling=pooling, use_layerwise=use_layerwise).to(device)
        # Removed: self.device = device # SentenceTransformer manages device internally

    def encode(self, texts, batch_size=32, convert_to_tensor=True, **kwargs):
        self.model.eval()
        all_embeddings = []
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                emb = self.model(batch).to(self.device)
                all_embeddings.append(emb)
        all_embeddings = torch.cat(all_embeddings, dim=0)
        return all_embeddings if convert_to_tensor else all_embeddings.cpu().numpy()

from mteb import get_tasks
from sentence_transformers import InputExample
from datasets import load_dataset # Import load_dataset

# Load NFCorpus task (still needed for task-specific info if any)
tasks = get_tasks(tasks=["NFCorpus"])
task = tasks[0]

# print("Task loaded:", task.description["name"]) # Removed line causing AttributeError

# Load dataset splits directly using datasets library from Hugging Face Hub
try:
    dataset = load_dataset("mteb/NFCorpus")
    print("\nDataset loaded successfully from Hugging Face Hub.")
    print(dataset.keys())  # Now this should work if loading was successful
except Exception as e:
    print(f"\nCould not load dataset from 'mteb/NFCorpus'. Please check the dataset name and availability.")
    print(e)
    dataset = None # Ensure dataset is None if loading fails


# Check train split
if dataset and "train" in dataset:
    train_data = dataset["train"]
    print("\n'train' split found.")
elif dataset and "test" in dataset: # fallback (some tasks only have test)
    train_data = dataset["test"]
    print("\n'train' split not found, using 'test' split as fallback.")
else:
    train_data = None
    print("\nCould not find 'train' or 'test' split in the dataset.")

len(train_data)

# ----------------------
# Build model + optimizer
# ----------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
model = CustomSentenceTransformer(pooling="attention", use_layerwise=True, device=device)

optimizer = optim.AdamW(model.model.parameters(), lr=2e-5)